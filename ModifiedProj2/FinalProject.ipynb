{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function/Class Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:40.222745Z",
     "start_time": "2025-05-02T03:09:40.220295Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './data/raw' # Where all the raw stories are\n",
    "TRAIN_FILE = '../data/dataset.jsonl'\n",
    "TEST_FILE = '../data/dataset.jsonl'\n",
    "CORPUS_FILE = 'corpus.txt' # Where all the raw data will be stored\n",
    "MODEL_WEIGHTS_DIR = \"final_model_weights.pth\"\n",
    "\n",
    "TOKENIZER_PREFIX = 'bpe_model' # Tokenizer name\n",
    "TOKENIZER_PATH = TOKENIZER_PREFIX + \".model\"\n",
    "PAD_TOKEN_ID = 5\n",
    "\n",
    "VOCAB_SIZE = 7017 # Based on project handout, limit of vocab tokens allowed\n",
    "MAX_TRAIN_SEQ_LEN = 1024\n",
    "MAX_GEN_SEQ_LEN = 1024\n",
    "\n",
    "\n",
    "#MODIFIABLE CONSTANTS FOR MODEL TRAINING START HERE\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = .002\n",
    "# Dictates creativity of the model, < 1 more deterministic, > 1 more creative/stochastic, 1 is no change from base model.\n",
    "TEMPERATURE = .9\n",
    "EARLY_EPOCH_STOP = 2\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = .2\n",
    "N_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:41.112207Z",
     "start_time": "2025-05-02T03:09:40.226780Z"
    }
   },
   "outputs": [],
   "source": [
    "from FinalProjHelper import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function takes in all of the batches from the dataset, which will be jagged arrays, and\n",
    "    inserts padding tokens <pad> such that all of the sequences are the same length. Note that the\n",
    "    Cross Entropy Loss criterion should specify to ignore the index 3 so it does not affect the training.\n",
    "\n",
    "    :param batch: The batch of prompts and completions that form a jagged array to be padded.\n",
    "    :return: The input and label batches properly padded.\n",
    "    \"\"\"\n",
    "\n",
    "    enc_input_batch, dec_input_batch, target_batch = zip(*batch)\n",
    "    enc_input_batch = nn.utils.rnn.pad_sequence(enc_input_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    dec_input_batch = nn.utils.rnn.pad_sequence(dec_input_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    target_batch = nn.utils.rnn.pad_sequence(target_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    return enc_input_batch, dec_input_batch, target_batch\n",
    "\n",
    "def train_model(model, device, tokenizer, model_type=\"\"):\n",
    "    \"\"\"\n",
    "    The main training code generalized for all of the models,\n",
    "    including evaluation metrics such as loss graphs, BLEU score, and Perplexity score.\n",
    "\n",
    "    :param model: The instantiated model needing training.\n",
    "    :param device: The device the model should be trained on, preferrably cuda.\n",
    "    :param tokenizer: The loaded tokenizer trained on the dataset being used for model training.\n",
    "    :return: The progression of training and testing losses.\n",
    "    \"\"\"\n",
    "    # Loading tokenizer file and getting most up to date vocab size\n",
    "    vocab_size = tokenizer.get_piece_size()\n",
    "\n",
    "    # Set up datasets from the given jsonl files for training\n",
    "    train_data = TextDatasetTED(TRAIN_FILE, tokenizer, MAX_TRAIN_SEQ_LEN)\n",
    "    test_data = TextDatasetTED(TEST_FILE, tokenizer, MAX_TRAIN_SEQ_LEN)\n",
    "\n",
    "    # Using pytorch DataLoaders for easy batching, shuffling, etc.\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Adding on a decaying learning rate to the optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(EPOCHS):\n",
    "        #Emptying cache and unused data on every epoch since CUDA would run out of memory otherwise\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "        #Want the model in training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for enc_input_ids, dec_input_ids, target_ids in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            enc_input_ids = enc_input_ids.to(device)\n",
    "            dec_input_ids = dec_input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the padding masks for proper training\n",
    "            enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "            dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "            # Getting the probability distributions for the prompts...\n",
    "            logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "            \"\"\"\n",
    "            For understanding this dimension change, understand that the logits are of\n",
    "            dimension (B,S,V) (see forward() function of base model for explanantion) and the targets are of\n",
    "            dimension (B,S) (where each entry is the correct token to predict for that position in the sequence).\n",
    "\n",
    "            For Cross Entropy Loss, we must have 1 prediction for each row in both tensors. Therefore, if we can reduce\n",
    "            each tensor such that it reads the last dimension for each row, the function will work. Aka we would have\n",
    "            dimension (B x S, V) (every row is a token's probability distribution) and\n",
    "            dimension (B x S) (every entry is that token's correct value, in chronological order with the logits)\n",
    "\n",
    "            The .view() function allows us to do this my making the last dimension of each tensor account for each entry.\n",
    "            \"\"\"\n",
    "            loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "\n",
    "            # Adjusting weights...\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Don't want the model to train on testing data, so .eval()\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate testing loss after training on this epoch to see performance on new data\n",
    "        with torch.no_grad():\n",
    "            for enc_input_ids, dec_input_ids, target_ids in test_loader:\n",
    "                enc_input_ids = enc_input_ids.to(device)\n",
    "                dec_input_ids = dec_input_ids.to(device)\n",
    "                target_ids = target_ids.to(device)\n",
    "\n",
    "                # Get the padding masks for proper training\n",
    "                enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "                dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "                # Getting the probability distributions for the prompts...\n",
    "                logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "                loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "            avg_test_loss = total_test_loss / len(test_loader)\n",
    "            test_losses.append(avg_test_loss)\n",
    "            scheduler.step(avg_test_loss)\n",
    "\n",
    "            # If our testing data starts getting worse over time, we can stop it early to reduce losses in accuracy based on a preset constant\n",
    "            if(avg_test_loss < best_test_loss):\n",
    "                best_test_loss = avg_test_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "\n",
    "        if(no_improve_epochs >= EARLY_EPOCH_STOP):\n",
    "            print(f\"No improvement in {EARLY_EPOCH_STOP} epochs, stopping...\")\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_WEIGHTS_DIR)\n",
    "    print(f\"Saved model weights to {MODEL_WEIGHTS_DIR}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Test Loss={avg_test_loss:.4f}\")\n",
    "    print(f\"Model Perplexity: {Perplexity(avg_train_loss):.4f} Model BLEU: {BLEU(model, tokenizer, test_loader):.4f}\")\n",
    "    plotLossOverEpochs(len(train_losses), train_losses, test_losses, model_type)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def plotLossOverEpochs(epochs, train_loss, test_loss, model_type=\"\"):\n",
    "        \"\"\"\n",
    "        Creates a plot showing the losses over time for a model.\n",
    "\n",
    "        :param epochs: The number of epochs the training took place over\n",
    "        :param train_loss: The losses of training over the epochs\n",
    "        :param test_loss: The losses of testing over the epochs\n",
    "        :param name: The name of the trained model being evaluated\n",
    "        \"\"\"\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(model_type + \" Loss per Epoch\")\n",
    "\n",
    "        x_range = range(1, epochs + 1)\n",
    "\n",
    "        plt.plot(x_range, train_loss)\n",
    "        plt.plot(x_range, test_loss)\n",
    "\n",
    "        plt.plot(x_range, train_loss, label=\"Training Loss\", color='blue')\n",
    "        plt.plot(x_range, test_loss, label=\"Testing Loss\", color='orange')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "#Because the loss is already cross entropy, we can just do the natural exponentiation of the loss\n",
    "#Loss here is the average loss across tokens\n",
    "def Perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "def BLEU(model, tokenizer, test_loader):\n",
    "    \"\"\"\n",
    "        Evaluates BLEU score of the entire model by getting probabilities.\n",
    "\n",
    "        :param epochs: The number of epochs the training took place over\n",
    "        :param train_loss: The losses of training over the epochs\n",
    "        :param test_loss: The losses of testing over the epochs\n",
    "        :param name: The name of the trained model being evaluated\n",
    "        :return: The overall BLEU scoring of the prompts and completions ran on\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    references = []\n",
    "    candidates = []\n",
    "    samples_processed = 0\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # We really just want the raw prompts and completions to get rid of unnecessary padding\n",
    "        for enc_input_ids, dec_input_ids, target_ids in test_loader:\n",
    "\n",
    "            enc_input_ids = enc_input_ids.to(DEVICE)\n",
    "            dec_input_ids = dec_input_ids.to(DEVICE)\n",
    "            target_ids = target_ids.to(DEVICE)\n",
    "\n",
    "            # Get the padding masks for proper training\n",
    "            enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "            dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "            # The model does teacher forcing predictions, which is exactly what we need to compare with the labels\n",
    "            logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "            # Taking the best token from each probability distribution for comparison against the labels\n",
    "            predicted_ids = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "            target_ids = target_ids.cpu().tolist()\n",
    "\n",
    "            for predicted, target in zip(predicted_ids, target_ids):\n",
    "\n",
    "                # Process 250 samples so it doesnt run forever\n",
    "                if samples_processed > 250:\n",
    "                    break\n",
    "\n",
    "                # For each prediction vector and label vector, decode it and add it to a list for BLEU scoring\n",
    "                pred_decode = tokenizer.decode(predicted, out_type=str)\n",
    "                reference = tokenizer.decode(target, out_type=str)\n",
    "\n",
    "                samples_processed += 1\n",
    "\n",
    "                candidates.append(pred_decode.split())\n",
    "                references.append([reference.split()])\n",
    "\n",
    "    # Compute the corpus-level BLEU score. For the purposes of this project, up to 3-gram comparisons were made\n",
    "    bleu_score = corpus_bleu(references, candidates, weights=(.25, .25, .25, .25), smoothing_function=smoothing_function)\n",
    "    return bleu_score\n",
    "\n",
    "def prompt_model(model, tokenizer, name):\n",
    "    while(1):\n",
    "        print(\"Prompt \" + name + \" (type q to quit): \")\n",
    "        prompt = input()\n",
    "        if prompt.lower() == \"q\":\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Your unit test: \" + model.generate(tokenizer,\n",
    "                                                       prompt,\n",
    "                                                       max_seq_length=MAX_GEN_SEQ_LEN,\n",
    "                                                       bos_token_id=BOS_TOKEN_ID,\n",
    "                                                       eos_token_id=EOS_TOKEN_ID,\n",
    "                                                       pad_token_id=PAD_TOKEN_ID,\n",
    "                                                       temperature=TEMPERATURE,\n",
    "                                                       device=DEVICE\n",
    "                                                      ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Preliminary Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:48.805791Z",
     "start_time": "2025-05-02T03:09:41.150695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def my_func ( lalala : str ) -> List [ int ] : \n",
      "     print ( \"foo bar\" ) \n",
      "     return [ 3 , 1 , 4 , 1 , 5 , 9 ] \n",
      " \n",
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "<BOS>\n",
      "<EOS>\n",
      "<PAD>\n",
      "\\n\n",
      "<INDENT>\n",
      "<DEDENT>\n",
      "▁(\n"
     ]
    }
   ],
   "source": [
    "run_spm = input(\"Train new tokenizer (needed if no .model file in project root)? (y/n): \").lower() == \"y\"\n",
    "tokenizer = Tokenizer(TOKENIZER_PREFIX)\n",
    "if run_spm:\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, sample_limit=None)\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "tokenizer.load()\n",
    "\n",
    "# Sanity Check:\n",
    "ids = tokenizer.encode(\"def my_func(lalala: str) -> List[int]:\\n    print(\\\"foo bar\\\")\\n    return [3, 1, 4, 1, 5, 9]\")\n",
    "code = tokenizer.decode(ids)\n",
    "print(code)\n",
    "\n",
    "for i in range(10):\n",
    "    print(tokenizer.sp.IdToPiece(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer ED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:48.968597Z",
     "start_time": "2025-05-02T03:09:48.813148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m train_new_model = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWould you like to train a new model? (y/n, where n is loading old weights): \u001b[39m\u001b[33m\"\u001b[39m).lower() == \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_new_model:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading old weights...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, device, tokenizer, model_type)\u001b[39m\n\u001b[32m     76\u001b[39m dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Getting the probability distributions for the prompts...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_pad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mFor understanding this dimension change, understand that the logits are of\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03mdimension (B,S,V) (see forward() function of base model for explanantion) and the targets are of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03mThe .view() function allows us to do this my making the last dimension of each tensor account for each entry.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m loss = criterion(logits.view(-\u001b[32m1\u001b[39m, vocab_size), target_ids.view(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/PythonTestAI/ModifiedProj2/FinalProjModels.py:44\u001b[39m, in \u001b[36mTransformerEDLanguageModel.forward\u001b[39m\u001b[34m(self, enc_input_ids, dec_input_ids, src_padding_mask, tgt_padding_mask)\u001b[39m\n\u001b[32m     41\u001b[39m src_embeds = \u001b[38;5;28mself\u001b[39m.embedding(enc_input_ids)\n\u001b[32m     42\u001b[39m tgt_embeds = \u001b[38;5;28mself\u001b[39m.embedding(dec_input_ids)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m src_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositional_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m tgt_embeds = \u001b[38;5;28mself\u001b[39m.positional_embed(tgt_embeds)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Creates an upper triangular matrix full of negative infinity. This stops the model from looking ahead in the sequence and\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# \"cheating,\" seeing the correct token and predicting that token every time. This leads to immediate overfitting and causes\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# prompt completion to repeat the last token it saw for every iteration.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/test/PythonTestAI/ModifiedProj2/FinalProjModels.py:136\u001b[39m, in \u001b[36mPositionalEncoding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from FinalProjModels import TransformerEDLanguageModel\n",
    "\n",
    "transformer_model = TransformerEDLanguageModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    enc_num_layers=NUM_LAYERS,\n",
    "    dec_num_layers=NUM_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_token_id=PAD_TOKEN_ID,\n",
    "    name=\"Transformer Encoder-Decoder\"\n",
    ").to(device)\n",
    "\n",
    "train_new_model = input(\"Would you like to train a new model? (y/n, where n is loading old weights): \").lower() == \"y\"\n",
    "if train_new_model:\n",
    "    train_model(transformer_model, device, tokenizer, transformer_model.name)\n",
    "else:\n",
    "    print(\"Loading old weights...\")\n",
    "    transformer_model.load_state_dict(torch.load(MODEL_WEIGHTS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Transformer Encoder-Decoder (type q to quit): \n"
     ]
    }
   ],
   "source": [
    "prompt_model(transformer_model, tokenizer, transformer_model.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
