{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function/Class Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:40.222745Z",
     "start_time": "2025-05-02T03:09:40.220295Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './data/raw' # Where all the raw stories are\n",
    "TRAIN_FILE = '../data/dataset.jsonl'\n",
    "TEST_FILE = '../data/dataset.jsonl'\n",
    "CORPUS_FILE = 'corpus.txt' # Where all the raw data will be stored\n",
    "MODEL_WEIGHTS_DIR = \"final_model_weights.pth\"\n",
    "\n",
    "TOKENIZER_PREFIX = 'bpe_model' # Tokenizer name\n",
    "TOKENIZER_PATH = TOKENIZER_PREFIX + \".model\"\n",
    "PAD_TOKEN_ID = 5\n",
    "\n",
    "VOCAB_SIZE = 7017\n",
    "MAX_TRAIN_SEQ_LEN = 1024\n",
    "MAX_GEN_SEQ_LEN = 1024\n",
    "\n",
    "\n",
    "#MODIFIABLE CONSTANTS FOR MODEL TRAINING START HERE\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = .002\n",
    "# Dictates creativity of the model, < 1 more deterministic, > 1 more creative/stochastic, 1 is no change from base model.\n",
    "TEMPERATURE = .9\n",
    "EARLY_EPOCH_STOP = 2\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = .2\n",
    "N_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:41.112207Z",
     "start_time": "2025-05-02T03:09:40.226780Z"
    }
   },
   "outputs": [],
   "source": [
    "from FinalProjHelper import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function takes in all of the batches from the dataset, which will be jagged arrays, and\n",
    "    inserts padding tokens <pad> such that all of the sequences are the same length. Note that the\n",
    "    Cross Entropy Loss criterion should specify to ignore the index 3 so it does not affect the training.\n",
    "\n",
    "    :param batch: The batch of prompts and completions that form a jagged array to be padded.\n",
    "    :return: The input and label batches properly padded.\n",
    "    \"\"\"\n",
    "\n",
    "    enc_input_batch, dec_input_batch, target_batch = zip(*batch)\n",
    "    enc_input_batch = nn.utils.rnn.pad_sequence(enc_input_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    dec_input_batch = nn.utils.rnn.pad_sequence(dec_input_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    target_batch = nn.utils.rnn.pad_sequence(target_batch, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    return enc_input_batch, dec_input_batch, target_batch\n",
    "\n",
    "def train_model(model, device, tokenizer, model_type=\"\"):\n",
    "    \"\"\"\n",
    "    The main training code generalized for all of the models,\n",
    "    including evaluation metrics such as loss graphs, BLEU score, and Perplexity score.\n",
    "\n",
    "    :param model: The instantiated model needing training.\n",
    "    :param device: The device the model should be trained on, preferrably cuda.\n",
    "    :param tokenizer: The loaded tokenizer trained on the dataset being used for model training.\n",
    "    :return: The progression of training and testing losses.\n",
    "    \"\"\"\n",
    "    # Loading tokenizer file and getting most up to date vocab size\n",
    "    vocab_size = tokenizer.get_piece_size()\n",
    "    \n",
    "    # Set up datasets from the given jsonl files for training\n",
    "    train_data = TextDatasetTED(TRAIN_FILE, tokenizer, MAX_TRAIN_SEQ_LEN)\n",
    "    test_data = TextDatasetTED(TEST_FILE, tokenizer, MAX_TRAIN_SEQ_LEN)\n",
    "\n",
    "    # Using pytorch DataLoaders for easy batching, shuffling, etc.\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Adding on a decaying learning rate to the optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(EPOCHS):\n",
    "        #Emptying cache and unused data on every epoch since CUDA would run out of memory otherwise\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "        #Want the model in training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for enc_input_ids, dec_input_ids, target_ids in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            enc_input_ids = enc_input_ids.to(device)\n",
    "            dec_input_ids = dec_input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the padding masks for proper training\n",
    "            enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "            dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "            # Getting the probability distributions for the prompts...\n",
    "            logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "            \"\"\"\n",
    "            For understanding this dimension change, understand that the logits are of\n",
    "            dimension (B,S,V) (see forward() function of base model for explanantion) and the targets are of\n",
    "            dimension (B,S) (where each entry is the correct token to predict for that position in the sequence).\n",
    "\n",
    "            For Cross Entropy Loss, we must have 1 prediction for each row in both tensors. Therefore, if we can reduce\n",
    "            each tensor such that it reads the last dimension for each row, the function will work. Aka we would have\n",
    "            dimension (B x S, V) (every row is a token's probability distribution) and\n",
    "            dimension (B x S) (every entry is that token's correct value, in chronological order with the logits)\n",
    "\n",
    "            The .view() function allows us to do this my making the last dimension of each tensor account for each entry.\n",
    "            \"\"\"\n",
    "            loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "\n",
    "            # Adjusting weights...\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Don't want the model to train on testing data, so .eval()\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate testing loss after training on this epoch to see performance on new data\n",
    "        with torch.no_grad():\n",
    "            for enc_input_ids, dec_input_ids, target_ids in test_loader:\n",
    "                enc_input_ids = enc_input_ids.to(device)\n",
    "                dec_input_ids = dec_input_ids.to(device)\n",
    "                target_ids = target_ids.to(device)\n",
    "\n",
    "                # Get the padding masks for proper training\n",
    "                enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "                dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "                # Getting the probability distributions for the prompts...\n",
    "                logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "                loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "            avg_test_loss = total_test_loss / len(test_loader)\n",
    "            test_losses.append(avg_test_loss)\n",
    "            scheduler.step(avg_test_loss)\n",
    "\n",
    "            # If our testing data starts getting worse over time, we can stop it early to reduce losses in accuracy based on a preset constant\n",
    "            if(avg_test_loss < best_test_loss):\n",
    "                best_test_loss = avg_test_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "\n",
    "        if(no_improve_epochs >= EARLY_EPOCH_STOP):\n",
    "            print(f\"No improvement in {EARLY_EPOCH_STOP} epochs, stopping...\")\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_WEIGHTS_DIR)\n",
    "    print(f\"Saved model weights to {MODEL_WEIGHTS_DIR}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Test Loss={avg_test_loss:.4f}\")\n",
    "    print(f\"Model Perplexity: {Perplexity(avg_train_loss):.4f} Model BLEU: {BLEU(model, tokenizer, test_loader):.4f}\")\n",
    "    plotLossOverEpochs(len(train_losses), train_losses, test_losses, model_type)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def plotLossOverEpochs(epochs, train_loss, test_loss, model_type=\"\"):\n",
    "        \"\"\"\n",
    "        Creates a plot showing the losses over time for a model.\n",
    "\n",
    "        :param epochs: The number of epochs the training took place over\n",
    "        :param train_loss: The losses of training over the epochs\n",
    "        :param test_loss: The losses of testing over the epochs\n",
    "        :param name: The name of the trained model being evaluated\n",
    "        \"\"\"\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(model_type + \" Loss per Epoch\")\n",
    "\n",
    "        x_range = range(1, epochs + 1)\n",
    "\n",
    "        plt.plot(x_range, train_loss)\n",
    "        plt.plot(x_range, test_loss)\n",
    "\n",
    "        plt.plot(x_range, train_loss, label=\"Training Loss\", color='blue')\n",
    "        plt.plot(x_range, test_loss, label=\"Testing Loss\", color='orange')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "#Because the loss is already cross entropy, we can just do the natural exponentiation of the loss\n",
    "#Loss here is the average loss across tokens\n",
    "def Perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "def BLEU(model, tokenizer, test_loader):\n",
    "    \"\"\"\n",
    "        Evaluates BLEU score of the entire model by getting probabilities.\n",
    "\n",
    "        :param epochs: The number of epochs the training took place over\n",
    "        :param train_loss: The losses of training over the epochs\n",
    "        :param test_loss: The losses of testing over the epochs\n",
    "        :param name: The name of the trained model being evaluated\n",
    "        :return: The overall BLEU scoring of the prompts and completions ran on\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    references = []\n",
    "    candidates = []\n",
    "    samples_processed = 0\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # We really just want the raw prompts and completions to get rid of unnecessary padding\n",
    "        for enc_input_ids, dec_input_ids, target_ids in test_loader:\n",
    "\n",
    "            enc_input_ids = enc_input_ids.to(DEVICE)\n",
    "            dec_input_ids = dec_input_ids.to(DEVICE)\n",
    "            target_ids = target_ids.to(DEVICE)\n",
    "\n",
    "            # Get the padding masks for proper training\n",
    "            enc_pad_mask = (enc_input_ids == PAD_TOKEN_ID)\n",
    "            dec_pad_mask = (dec_input_ids == PAD_TOKEN_ID)\n",
    "\n",
    "            # The model does teacher forcing predictions, which is exactly what we need to compare with the labels\n",
    "            logits = model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n",
    "\n",
    "            # Taking the best token from each probability distribution for comparison against the labels\n",
    "            predicted_ids = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "            target_ids = target_ids.cpu().tolist()\n",
    "\n",
    "            for predicted, target in zip(predicted_ids, target_ids):\n",
    "\n",
    "                # Process 250 samples so it doesnt run forever\n",
    "                if samples_processed > 250:\n",
    "                    break\n",
    "\n",
    "                # For each prediction vector and label vector, decode it and add it to a list for BLEU scoring\n",
    "                pred_decode = tokenizer.decode(predicted, out_type=str)\n",
    "                reference = tokenizer.decode(target, out_type=str)\n",
    "\n",
    "                samples_processed += 1\n",
    "\n",
    "                candidates.append(pred_decode.split())\n",
    "                references.append([reference.split()])\n",
    "\n",
    "    # Compute the corpus-level BLEU score. For the purposes of this project, up to 3-gram comparisons were made\n",
    "    bleu_score = corpus_bleu(references, candidates, weights=(.25, .25, .25, .25), smoothing_function=smoothing_function)\n",
    "    return bleu_score\n",
    "\n",
    "def prompt_model(model, tokenizer, name):\n",
    "    while(1):\n",
    "        print(\"Prompt \" + name + \" (type q to quit): \")\n",
    "        prompt = input()\n",
    "        if prompt.lower() == \"q\":\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Your unit test: \" + model.generate(tokenizer,\n",
    "                                                       prompt,\n",
    "                                                       max_seq_length=MAX_GEN_SEQ_LEN,\n",
    "                                                       bos_token_id=BOS_TOKEN_ID,\n",
    "                                                       eos_token_id=EOS_TOKEN_ID,\n",
    "                                                       pad_token_id=PAD_TOKEN_ID,\n",
    "                                                       temperature=TEMPERATURE,\n",
    "                                                       device=DEVICE\n",
    "                                                      ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Preliminary Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:48.805791Z",
     "start_time": "2025-05-02T03:09:41.150695Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/all.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(TOKENIZER_PREFIX)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_spm:\n\u001b[1;32m----> 4\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mtrain(vocab_size\u001b[38;5;241m=\u001b[39mVOCAB_SIZE, sample_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(DEVICE)\n\u001b[0;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[1;32mc:\\Users\\joshm\\OneDrive\\Desktop\\AI Foundations\\FinalProject\\FinalProjectGithub\\PythonTestAI\\ModifiedProj2\\FinalProjHelper.py:109\u001b[0m, in \u001b[0;36mTokenizer.train\u001b[1;34m(self, vocab_size, sample_limit)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7017\u001b[39m, sample_limit: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Make tokenizer\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     all_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/all.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data_file:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_file):\n\u001b[0;32m    111\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m sample_limit \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m sample_limit:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/all.jsonl'"
     ]
    }
   ],
   "source": [
    "run_spm = input(\"Train new tokenizer (needed if no .model file in project root)? (y/n): \").lower() == \"y\"\n",
    "tokenizer = Tokenizer(TOKENIZER_PREFIX)\n",
    "if run_spm:\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, sample_limit=None)\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "tokenizer.load()\n",
    "\n",
    "# Sanity Check:\n",
    "ids = tokenizer.encode(\"def my_func(lalala: str) -> List[int]:\\n    print(\\\"foo bar\\\")\\n    return [3, 1, 4, 1, 5, 9]\")\n",
    "code = tokenizer.decode(ids)\n",
    "print(code)\n",
    "\n",
    "for i in range(10):\n",
    "    print(tokenizer.sp.IdToPiece(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer ED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:09:48.968597Z",
     "start_time": "2025-05-02T03:09:48.813148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m train_new_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWould you like to train a new model? (y/n, where n is loading old weights): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_new_model:\n\u001b[1;32m---> 17\u001b[0m     train_model(transformer_model, device, tokenizer, transformer_model\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading old weights...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 79\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, device, tokenizer, model_type)\u001b[0m\n\u001b[0;32m     76\u001b[0m dec_pad_mask \u001b[38;5;241m=\u001b[39m (dec_input_ids \u001b[38;5;241m==\u001b[39m PAD_TOKEN_ID)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Getting the probability distributions for the prompts...\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(enc_input_ids, dec_input_ids, enc_pad_mask, dec_pad_mask)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mFor understanding this dimension change, understand that the logits are of\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03mdimension (B,S,V) (see forward() function of base model for explanantion) and the targets are of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03mThe .view() function allows us to do this my making the last dimension of each tensor account for each entry.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), target_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\joshm\\OneDrive\\Desktop\\AI Foundations\\FinalProject\\FinalProjectGithub\\PythonTestAI\\ModifiedProj2\\FinalProjModels.py:42\u001b[0m, in \u001b[0;36mTransformerEDLanguageModel.forward\u001b[1;34m(self, enc_input_ids, dec_input_ids, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_input_ids, dec_input_ids, src_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    The TransformerED implementation of layer propagation. Takes the input to the system with the target data and runs it through a traditional\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    encoder-decoder network with embeddings.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     src_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(enc_input_ids)\n\u001b[0;32m     43\u001b[0m     tgt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(dec_input_ids)\n\u001b[0;32m     45\u001b[0m     src_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embed(src_embeds)\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joshm\\miniconda3\\envs\\AI_Foundation_Proj2\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from FinalProjModels import TransformerEDLanguageModel\n",
    "\n",
    "transformer_model = TransformerEDLanguageModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    enc_num_layers=NUM_LAYERS,\n",
    "    dec_num_layers=NUM_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_token_id=PAD_TOKEN_ID,\n",
    "    seq_len=MAX_TRAIN_SEQ_LEN,\n",
    "    name=\"Transformer Encoder-Decoder\"\n",
    ").to(device)\n",
    "\n",
    "train_new_model = input(\"Would you like to train a new model? (y/n, where n is loading old weights): \").lower() == \"y\"\n",
    "if train_new_model:\n",
    "    train_model(transformer_model, device, tokenizer, transformer_model.name)\n",
    "else:\n",
    "    print(\"Loading old weights...\")\n",
    "    transformer_model.load_state_dict(torch.load(MODEL_WEIGHTS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Transformer Encoder-Decoder (type q to quit): \n"
     ]
    }
   ],
   "source": [
    "prompt_model(transformer_model, tokenizer, transformer_model.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
